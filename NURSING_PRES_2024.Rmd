---
title: "NURSING Web Scraping Presentation 2024"
output: html_document
author: Steve Pittard
date: "2024-04-12"
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rvest)
library(tidyr)
library(dplyr)
library(ggplot2)
```

## Introduction

This notebook contains updated code for the [Web Scraping with
R](https://steviep42.github.io/webscraping/book/) on my github which has
lots of information although the underlying code requires frequent
updating in response to the inevitable changes made by the various
websites that we are trying to parse with our R code.

This is a job in itself but is also demonstrative of the fact that
websites frequently change, go away, or are updated to reflect new
ownership or direction. So today's code might work well today but
perhaps not in a month's time! Such is the nature of scraping web data
programmatically.

```
https://www.33rdsquare.com/is-web-scraping-legal/
```

Some basic lessons here are that when searching the web for data
intended for use with R you should always see if there is an approved
API or package BEFORE resorting to web scraping. Check the Task Views
part of [CRAN](https://cran.r-project.org/)

## Why Bother ?

Web scraping seems to be an arbitrary place to start but it's actually a
powerful technique with which to have familiarity given that so much
info is available on the internet. That doesn't mean that the authors
want you to be downloading and using their work although that is
precisely what has happened with chatGPT and other Large Language Models
that have been trained on publicly available sources.

That aside, it's best practice to view access conditions for any web
site you are thinking about scraping information from. Some times they
have explicit conditions whereas other times they do not. Actively
scraping a web site, especially one with lots of information, can
"hammer" the server which can make access slower for the typical
interactive user.

Increasingly, sites that are the target of scraping activity will
provide APIs for more convenient and less intrusive access. Sometimes
these are free sometimes not. If you are lucky then the R community had
developed a package that wraps in the API so that anything you get back
is usually in the form of a data frame or at least a list.

This way you do not have to do the scraping. Nonetheless, this is a good
place to start since it represents the lowest common denominator of
getting information form the Internet in a programmatic fashion.

## Quick rvest Tutorial

The word **rvest** is a phonetic approximation of the work **harvest**
as in "**harvesting**" information. Cute huh?

Now let’s do a quick rvest tutorial. There are several steps involved in
using rvest which are conceptually quite straightforward:

1)  Identify a URL to be examined for content
2)  Use Selector Gadet, xPath, or Google Insepct to identify the
    “selector” This will be a paragraph, table, hyper links, images
3)  Load rvest
4)  Use read_html to “read” the URL
5)  Pass the result to html_nodes to get the selectors identified in
    step number 2
6)  Get the text or table content

First, we can generate html with rvest although that isn't necessarily
what we want to do but it could be helpful in understanding HTML which
underlies the web.

```{r}
library(rvest)
minimal_html("<p>This is a paragraph</p>
               <table>
                  <tr><td>Hi</td><td>There</td></tr>
                  <tr><td>Hi</td><td>There</td></tr>
                  <tr><td>Hi</td><td>There</td></tr>
               </table>")
```

The above result is what HTML will look like once you've parsed it with
rvest but more on that soon.

```{r}
minimal_html("<p>This is a paragraph</p>
               <table>
                  <tr><td>Hi</td><td>There</td></tr>
               </table>") %>% 
                html_element("p") %>% html_text()
```

Anyway, Check out this Wikpedia page for World Population information.
It's not the only one but it's good enough to show the power of the
**rvest** package:

```         
https://en.wikipedia.org/wiki/World_population
```

Let's see how we might extract information from the above web page.

```{r}
library(rvest)
url <- "https://en.wikipedia.org/wiki/World_population"

(paragraphs <- read_html(url) %>% html_nodes("p"))
```

Then we might want to actually parse out those paragraphs into text:

```{r}
url <- "https://en.wikipedia.org/wiki/World_population"
paragraphs <- read_html(url) %>% html_nodes("p") %>% html_text()
paragraphs[1:10]
```

Get some other types of HTML obejects. Let’s get all the hyperlinks to
other pages

```{r}
read_html(url) %>% html_elements("a") 
```

So we could look at the individual sections available on the web page

```{r}
read_html(url) %>% 
  html_elements("a") %>% 
  html_attr("href") %>%
  grep("^#",.,value=TRUE) %>%
  grep("^#(?!cite_)", ., value = TRUE, perl = TRUE)
```

So what about the tables on this page which have various pieces of
information on population. Let's say we want to get one of more of the
tables that describe the most populous countries. Which one do you want?
Well let's see how many tables there are.

```{r}
url <- "https://en.wikipedia.org/wiki/World_population"
ten_most_df <- read_html(url) 

ten_most_df %>% html_elements("table")

```

So I think table number 5 is the one I'm after. Note, as I've done this
before, I've got a good idea that this is true but over the years I've
been using this table as an example I've found that the editors of this
page will update it and add new tables which throws off my estimate of
which table to get. I have no control over this nor do you.

```{r}
ten_most_populous <- ten_most_df %>% 
  html_nodes("table") %>% `[[`(5) %>% html_table()

# Let's get just the first three columns
ten_most_populous <- ten_most_populous[,c(1,2,4)]

# Get some content - Change the column names
names(ten_most_populous) <- c("Country_Territory","Population","Date")

# Show the data frame we worked so hard to get
ten_most_populous
```

So now that we have what appears to be a real data frame we can start to
do things with it like plot stuff.

```{r}


# Do reformatting on the columns to be actual numerics where appropriate
ten_most_populous <- ten_most_populous %>%
  mutate(Population = as.numeric(gsub(",", "", Population)) / 1e+06)  %>%
  arrange(Population)  # Arrange the data by population size

ggplot(ten_most_populous, aes(x = reorder(Country_Territory, Population), y = Population)) +
  geom_bar(stat = "identity") +
  labs(y = "Population (millions)") +
  ggtitle("Top 10 Most Populous Countries") +
  labs(x="Country","Population / 1,000,000",caption="Source: https://en.wikipedia.org/wiki/World_population") + 
  coord_flip() + theme_bw()
```

Perhaps there is a way we can get the correct table using one of the
plugins I mentioned previously. Well it's mentioned in my online book in
the [Useful Tools
section](https://steviep42.github.io/webscraping/book/index.html#useful-tools).
Let's see what we can do.

```{r}
path <- "/html/body/div[2]/div/div[3]/main/div[3]/div[3]/div[1]/table[5]"

ten_most_df <- read_html(url) %>%
  html_nodes(xpath=path) %>%
  html_table()

ten_most_df
```

The above looks like the right table and we were able to target the
table directly using a well-formed XPath expression.

## More Real Examples

Let's look at some BitCoin data. It's an interesting topic because
people have gotten rich playing the Crypto Currency Market. Let’s check
the price of bitcoins. You want to be rich don’t you ?

```         
https://coinmarketcap.com/all/views/all/
```

Let's see how many tables there are on this page:

```{r}
library(rvest)
url <- "https://coinmarketcap.com/all/views/all/"
bc <- read_html(url)

bc %>% 
  html_nodes('table')
```

So we could just read each of them and check them out. Let's get the
third one.

```{r}
bc_table <- bc %>% 
  html_nodes('table') %>% 
  html_table() %>% .[[3]]
 # We get back a one element list that is a data frame
 str(bc_table,0)
 
bc_table
```

That looks like the one but we don't want everything. Let's get the
Name, Symbol and the Price.

```{r}
bc_table <- bc_table[,c(2:3,5)]
 head(bc_table)
```

We have to clean it up because it's all characters.

```{r}
# The data is "dirty" and has characers in it that need cleaning
bc_table <- bc_table %>% mutate(Price=gsub("\\$","",Price))
bc_table <- bc_table %>% mutate(Price=gsub(",","",Price))
bc_table <- bc_table %>% mutate(Price=round(as.numeric(Price),2))

# There are four rows wherein the Price is missing NA
bc_table <- bc_table %>% filter(complete.cases(bc_table))

# Let's get the Crypto currencies with the Top 10 highest prices 
top_10 <- bc_table %>% arrange(desc(Price)) %>% head(10)
top_10
```

Let's make a barplot

```{r}
# Next we want to make a barplot of the Top 10
ylim=c(0,max(top_10$Price)+10000)
main="Top 10 Crypto Currencies in Terms of Price"
bp <- barplot(top_10$Price,col="aquamarine",
              ylim=ylim,main=main)
axis(1, at=bp, labels=top_10$Symbol,  cex.axis = 0.7)
grid()
```

Uggh, BTC makes all the others look like they don't exist at least in
terms of price.

```{r}
# Let's take the log of the price
ylim=c(0,max(log(top_10$Price))+5)
main="Top 10 Crypto Currencies in Terms of log(Price)"
bp <- barplot(log(top_10$Price),col="aquamarine",
              ylim=ylim,main=main)
axis(1, at=bp, labels=top_10$Symbol,  cex.axis = 0.7)
grid()
```

## IMDB

Look at this example from IMDb (Internet Movie Database). According to
Wikipedia:

IMDb (Internet Movie Database)[2] is an online database of information
related to films, television programs, home videos, video games, and
streaming content online – including cast, production crew and personal
biographies, plot summaries, trivia, fan and critical reviews, and
ratings. We can search or refer to specific movies by URL if we wanted.
For example, consider the following link to the “Lego Movie”:

```         
http://www.imdb.com/title/tt1490017/
```

In terms of scraping information from this site we could do that using
the rvest package. Let’s say that we wanted to capture the rating
information which is 7.8 out of 10. We could use the xPath Tool to zone
in on this information.

```{r}
url <- "http://www.imdb.com/title/tt1490017/"
lego_movie <- read_html(url)

lego_movie
```

So this reads the page from which we must isolate the rating value. If
we look at the webpage with the XPath Extension or the Developer tools
we can find a couple of ways to target the rating value.

```{r}
# Scrape the website for the movie rating
url <- "http://www.imdb.com/title/tt1490017/"
lego_movie <- read_html(url)

# The Xpath looks like:
# "/html/body/div[2]/main/div/section[1]/section/div[3]/section/section/div[2]/div[2]/div/div[1]/a/span/div/div[2]/div[1]/span[1]"

# The Dev tools shows us "span.sc-bde20123-1.cMEQkK"

rating <- lego_movie %>%
  html_nodes("span.sc-bde20123-1.cMEQkK") %>%
  html_text() 

rating
```

Let’s access the summary section of the link.

```{r}
path <- "/html/body/div[2]/main/div/section[1]/section/div[3]/section/section/div[3]/div[2]/div[1]/section/p/span[3]"

mov_summary <- lego_movie %>%
  html_nodes(xpath=path) %>%
  html_text() 

mov_summary
```

## FDA

Check out the information available at this link

<https://www.fda.gov/drugs/novel-drug-approvals-fda/novel-drug-approvals-2015>

So to process the table on the FDA page we need to determine what it is
we are interested in. For one, the whole table which contains all the
approved drugs for the given year. It also has some informational links
to the drugs themselves and the clinical trial process for a given drug.

Let's start by pulling all the information for the year 2015 and putting
it into a data frame. this will require some knowledge on how to
identify the appropriate elements of interest along with the ability to
use **rvest** to get that info. This can many times be a trial-and-error
thing but if you use the the XPath plugin for your browser it becomes
easier.

```{r}
library(tidyverse)
library(lubridate)
library(rvest)

url <- "https://www.fda.gov/drugs/novel-drug-approvals-fda/novel-drug-approvals-2015"
init <- read_html(url)
fda <- read_html(url) %>% 
  html_table() 

fda <- do.call(rbind,fda)
names(fda) <- c("No","Name","Active_Ingredient","Date","Approved_Use")
fda$Date <- mdy(fda$Date)
```
### How Many Approvals in 2015 ?

Let's see how many were processed/approved in each month of the year. I
don't think there is any discernable patterm here. It does look like
more were approved in the second part of the year but that might be
conincidence.

```{r}
fda %>% 
  mutate(Months=months(Date,abbreviate=T)) %>%
  mutate(Months=factor(Months,level=month.abb)) %>%
  ggplot(aes(x=Months)) + geom_bar() +
  labs(x="Month",y="Number of Approved Drugs",title="Number of Approved Drugs / Month in Year 2015",
       caption="https://www.fda.gov/drugs/novel-drug-approvals-fda/novel-drug-approvals-2015") +
  theme_bw()
```

### Create a Function 

So one good practice to adopt in your programming is to turn working code into a function. We could do this with the code to get the table and turn it into a data frame. So, given a URL, we can get the table of interest from it. It's pretty easy here to do that. One thing that we know is that the FDA URL looks like this:


```         
https://www.fda.gov/drugs/novel-drug-approvals-fda/novel-drug-approvals-2015
```

If we look at that page we can see that there are other years with drug approvals all the way up to 2024. The only thing that changes in the URL is the last 4 digits. So we can assume a common stem of:

```         
https://www.fda.gov/drugs/novel-drug-approvals-fda/novel-drug-approvals-
```

which we will use inside our function. We'll pass only the year of interest to the function - in this case 2015.

```{r}
fda_get <- function(year=2015) {
#
# INPUT: year - a valid year for the FDA Drug site
# OUTPUT: A data frame or tibble represented the table found on the site

  url <- "https://www.fda.gov/drugs/novel-drug-approvals-fda/novel-drug-approvals-"
  url <- paste(url,year,sep="")
  
  fda <- read_html(url) %>% 
  html_table() 

  fda <- do.call(rbind,fda)
  names(fda) <- c("No","Name","Active_Ingredient","Date","Approved_Use")
  fda$Date <- mdy(fda$Date)

  return(fda)
}
```


So now we have a function we can call. Let's make sure it works

```{r}
yr_2015 <- fda_get(2015)
head(yr_2015)
```

Plot it like before:

```{r}
yr_2015 %>% 
  mutate(Months=months(Date,abbreviate=T)) %>%
  mutate(Months=factor(Months,level=month.abb)) %>%
  ggplot(aes(x=Months)) + geom_bar() +
  labs(x="Month",y="Number of Approved Drugs",title="Number of Approved Drugs / Month in Year 2015",
       caption="https://www.fda.gov/drugs/novel-drug-approvals-fda/novel-drug-approvals-2015") +
  theme_bw()
```

We could turn the plotting code into a function for generality.

```{r}
fda_bar <- function(df=fda_df) {
  Year <- year(df$Date[1])
  df %>% 
    mutate(Months=months(Date,abbreviate=T)) %>%
    mutate(Months=factor(Months,level=month.abb)) %>%
    ggplot(aes(x=Months)) + geom_bar() +
    labs(x="Month",y="Number of Approved Drugs",
         title=paste("Number of Approved Drugs / Month in Year",Year),
         caption=paste0("https://www.fda.gov/drugs/novel-drug-approvals-fda/novel-drug-approvals-",Year)) +
    theme_bw()
}
```


```{r}
fda_bar(yr_2015)
```

Will it work for another year?

```{r}
yr_2016 <- fda_get(2016)
fda_bar(yr_2016)
```

```{r}
fda_bar(fda_get(2017))
```

### Generalize the Code to All Years

Well this is great but let's go ahead and get all the approved drugs for all available years. it turns out that there is a problem with the table for 2020 so we will ignore that one for a while. We can use a little **apply** magic from R to leverage our **fda_get** function:

```{r}
all_fda <- do.call(rbind,lapply(c(2015:2019,2021:2024),fda_get))
```

```{r}
all_fda %>% 
    filter(complete.cases(.)) %>%
    mutate(Months=months(Date,abbreviate=T)) %>%
    mutate(Months=factor(Months,level=month.abb)) %>%
    mutate(Year=as.character(year(Date))) %>% 
    ggplot(aes(x=Months,fill=Year)) + geom_bar() +
    labs(x="Month",y="Number of Approved Drugs",
         title=paste("Number of Approved Drugs / Month in Year",Year)) +
    theme_bw()
```
### Get Drug Trial Snapshot Information

So let's take this farther by getting the Drug Trial links in the 5th column of 
the web page of interest. The process as a whole isn't going to be pretty because it 
requires experience with regular expressions, string manipulation and XML concepts but 
it's entirely doable. This is a "real life" example. Not every scraping task will get this involved. 

First, using the approved drug table on the link, we'll create a data frame with the name of the drug as well as the clinical trials info associated with it

```{r}
fda_get2 <- function(year=2015) {

  url <- "https://www.fda.gov/drugs/novel-drug-approvals-fda/novel-drug-approvals-"
  url <- paste(url,year,sep="")
  
  fda <- read_html(url) %>% 
    html_nodes("table tr td:nth-child(5) a") %>%  
    html_attr("href")

  # Extract the part after "snapshot" using regular expressions
  snapshot_info <- gsub('.*snapshot[s]?-(.*)', '\\1', fda)

  # We'll need the full link for later parsing
  full_links <- paste("https://www.fda.gov",fda,sep="")

  # Create a data frame with the extracted part and the entire link
  fda <- data.frame(
                    Snapshot_Info = snapshot_info,
                    Full_Link = full_links
                    )
  return(fda)
}
```

So actually get the info. In this case just for the year 2015

```{r}
fda_links <- fda_get2()
head(fda_links)
```

So now that we have a data frame of drug names and a link to the clinical trials info we can perhaps loop over every row in the data frame to get more information about each drug. We can also normalize the drug name to be uppercase.


```{r}
drug_names <- vector()

# Loop through all the links
for (ii in 1:nrow(fda_links)) {
  
  # Read the links
  doc <- read_html(fda_links[ii,2])
  
  # Get the drug header line
  drug_name <- doc %>% 
    html_elements(xpath="/html/body/div[2]/div[1]/div/main/article/header/section/div/h1") %>%     html_text()
  
  # We have to parse out the actual name from a larger string.
  drug_name <- trimws(strsplit(drug_name,":")[[1]][2])

  # More "word" surgery  
  first_word <- toupper(strsplit(drug_name," ")[[1]][1])

  # Stash the name in a vector
  
  drug_names[ii] <- first_word
}

drug_names
```

Now let's get an amplified description of what each drug is for. We do this by using 
rvest to "read" the URL and pick out the information under the "What is This Drug For"
heading on the page. We'll just extract the first paragraph.

```{r}
#Set up an empty vector into which we will put the descriptions
descriptions <- vector()

# Like above, let's loop through all the links we parsed earlier
for (ii in 1:nrow(fda_links)) {
  
  # 'Read' the link
  doc <- read_html(fda_links[ii,2])
  
  # We can use some XML magic to get the first paragraph after the heading of interest
  first_paragraph <- doc %>% 
    html_nodes(
      xpath = "/html/body/div[2]/div[1]/div/main/article/div/h4[1]/following-sibling::p[1]"
      ) %>% 
    html_text()
    
  descriptions[ii] <- first_paragraph
}

```

Let's check out the descriptions
```{r}
cat(descriptions[1:5],sep=" \n\n")
```

So now we could create a data frame for later processing

```{r}
drug_descriptions <- tibble(name=drug_names,
                            description=descriptions)
drug_descriptions
```

So now you are ready to begin applying some of the techniques described below for doing something like Topic Analysis. You would also probably need to use the above code segments
to create a function to get all the info available for multiple years of approved drugs. The above example is just for the year 2015. But the hard work is done. 


## PubMed

Pubmed provides a rich source of information on published scientific
literature. There are tutorials on how to leverage its capabilities but
one thing to consider is that MESH terms are a good starting place since
the search is index-based. MeSH (Medical Subject Headings) is the NLM
controlled vocabulary thesaurus used for indexing articles for PubMed.

It’s faster and more accurate so you can first use the MESH browser to
generate the appropriate search terms and add that into the Search
interface. The MESH browser can be found at
<https://www.ncbi.nlm.nih.gov/mesh/>

What we do here is get the links associated with each publication so we
can then process each of those and get the abstract associated with each
publication.

```{r}
url<-"https://www.ncbi.nlm.nih.gov/pubmed/?term=%22hemodialysis%2C+home%22+%5BMeSH+Terms%5D"
```

So how many results do we get form our search ?

```{r}
how_many <- read_html(url) %>%
  html_nodes(xpath="/html/body/main/div[9]/div[2]/div[2]/div[2]/div/label[2]") %>%
  html_text()

how_many
```

For now we'll just parse the first page.

```{r}
results <- read_html(url) %>% 
  html_nodes("a") %>% 
  html_attr("href") %>%
  grep("/[0-9]{6,6}",.,value=TRUE) %>% unique(.)

results
```

So now we could loop through these links and get the abstracts for these
results. It looks that there are approximately 20 results per page. As
before we would have to dive in to the underlying structure of the page
to get the correct HTML path names or we could just look for Paragraph
elements and pick out the links that way.

```{r}
text.vec <- vector()

for (ii in 1:length(results)) {
  if (results[ii]!="/4346131/") { # I found out that 4346131 has no abstract so I need to exlude it
    string <- paste0("https://pubmed.ncbi.nlm.nih.gov",results[ii])
    print(string)
    text.vec[ii] <- read_html(string) %>% 
       html_nodes(xpath="/html/body/div[5]/main/div[2]/div/p") %>% 
       html_text()
  }
}

# Eliminate lines with newlines characters
final.vec <- gsub("\n","",text.vec)
final.vec <- gsub("^\\s+","",final.vec)

#final.vec <- text.vec[grep("^\n",text.vec,invert=TRUE)]
```

```{r}
final.vec
```

Well that was tedious. And we processed only the first page of results.
How do we “programmatically” hit the “Next” Button at the bottom of the
page ? This is complicated by the fact that there appears to be some
Javascript at work that we would have to somehow interact with to get
the URL for the next page. We could use the RSelenium package.

## APIs

APIs provide a more convenient way to get data without having to scrape
it. These rely on a knowledge of interacting with REST interfaces which
usually return information in the form of JSON or XML.

### Easy PubMed

So there is an R package called EasyPubMed that helps ease the access of
data on the Internet. The idea behind this package is to be able to
query NCBI Entrez and retrieve PubMed records in XML or TXT format.

The PubMed records can be downloaded and saved as XML or text files if
desired. According to the package authours, “Data integrity is enforced
during data download, allowing to retrieve and save very large number of
records effortlessly.” The bottom line is that you can do what you want
after that. Let’s look at an example involving home hemodialysis

```{r}
library(easyPubMed)
```

```{r}
my_query <- 'hemodialysis, home" [MeSH Terms]'
my_entrez_id <- get_pubmed_ids(my_query)

my_abstracts <- fetch_pubmed_data(my_entrez_id)
my_abstracts <- custom_grep(my_abstracts,"AbstractText","char")

my_abstracts[1:10]
```

### OMDB

Let’s look at the IMDB page which catalogues lots of information about
movies. Just got to the web site and search although here is an example
link. <https://www.imdb.com/title/tt0076786/?ref_=fn_al_tt_2> In this
case we would like to get the summary information for the movie. So we
would use Selector Gadget or some other method to find the XPATH or CSS
associated with this element. This relies on signing up for a "key" to
access OMDB.

```{r}
library(RJSONIO)
url <- "http://www.omdbapi.com/?apikey=f7c004c&t=The+Lego+Movie"

# Fetch the URL via fromJSON
movie <- fromJSON(url)

str(movie)

```

```{r}
sapply(movie$Ratings,unlist)
```

Let’s Get all the Episodes for Season 1 of Game of Thrones

```{r}
url <- "http://www.omdbapi.com/?apikey=f7c004c&t=Game%20of%20Thrones&Season=1"
movie <- fromJSON(url)
str(movie,1)
```

```{r}
episodes <- data.frame(do.call(rbind,movie$Episodes),stringsAsFactors = FALSE)
episodes
```

### OMDB Package

Wait a minute. Looks like someone created an R package that wraps all
this for us. It is called omdbapi

```{r eval=FALSE}
library(omdbapi)
# The first time you use this you will be prompted to enter your
 # API key
movie_df <- search_by_title("Star Wars", page = 2)
(movie_df <- movie_df[,-5])
```

## RSelenium

R Selenium represents an advanced tool for pulling data off of a website
that uses JavaScript to manage navigation. It's not necessarily the
easiest tool to use although it is extremely power anf flexible for
programmatically manipulating websites to get information.

```{r}
library(RSelenium)
library(rvest)
library(tm)
library(SentimentAnalysis)
library(wordcloud)

```

```{r}
url <- "https://www.dailystrength.org/group/dialysis"

# The website has a "show more" button that hides most of the patient posts
# If we don't find a way to programmatically "click" this button then we can
# only get a few of the posts and their responses. To do this we need to
# use the RSelenium package which does a lot of behind the scenes work

# See https://cran.r-project.org/web/packages/RSelenium/RSelenium.pdf
# http://brazenly.blogspot.com/2016/05/r-advanced-web-scraping-dynamic.html

# Open up a connection 

# rD <- rsDriver()
# So, you might have to specify the version of chrome you are using
# For someone reason this seems now to be necessary (11/4/19)

rD <- rsDriver(browser=c("firefox"),version="latest")
remDr <- rD[["client"]]
remDr$navigate(url)
```

```{r}
loadmorebutton <- remDr$findElement(using = 'css selector', "#load-more-discussions")

```

```{r}
loadmorebutton$clickElement()
```

```{r}
page_source <- remDr$getPageSource()

# So let's parse the contents

comments <- read_html(page_source[[1]])

cumulative_comments <- vector()

links <- comments %>% html_nodes(css=".newsfeed__description")  %>% 
  html_node("a") %>% html_attr("href")

full_links <- paste0("https://www.dailystrength.org",links)

if (length(grep("NA",full_links)) > 0) {
  full_links <- full_links[-grep("NA",full_links)]
}

ugly_xpath <- '//*[contains(concat( " ", @class, " " ), concat( " ", "comments__comment-text", " " ))] | //p'

for (ii in 1:length(full_links)) {
  text <- read_html(full_links[ii]) %>% 
    html_nodes(xpath=ugly_xpath) %>% 
    html_text() 
  length(text) <- length(text) - 1
  text <- text[-1]
  
  text
  
  cumulative_comments <- c(cumulative_comments,text)
}

```

```{r}
cumulative_comments
```

## Bag of Words Sentiment Analysis

```{r}
url <- "https://millercenter.org/the-presidency/presidential-speeches/march-4-1865-second-inaugural-address"
library(rvest)
lincoln_doc <- read_html(url) %>%
                    html_nodes(".view-transcript") %>%
                    html_text()
lincoln_doc
```

There are probably lots of words that don’t really “matter” or
contribute to the “real” meaning of the speech.

```{r}
word_vec <- unlist(strsplit(lincoln_doc," "))
word_vec[1:20]
```

```{r}
sort(table(word_vec),decreasing = TRUE)[1:10]
```

```{r}
# Remove all punctuation marks
word_vec <- gsub("[[:punct:]]","",word_vec)
stop_words <- c("the","to","and","of","the","for","in","it",
                "a","this","which","by","is","an","hqs","from",
                "that","with","as")
for (ii in 1:length(stop_words)) {
    for (jj in 1:length(word_vec)) {
      if (stop_words[ii] == word_vec[jj]) {
          word_vec[jj] <- ""
} }
}
word_vec <- word_vec[word_vec != ""]
sort(table(word_vec),decreasing = TRUE)[1:10]
```

```{r}
word_vec[1:30]
```

## Tidy Text

So the tidytext package provides some accomodations to convert your body
of text into individual **tokens** which then simplfies the removal of
less meaningful words and the creation of word frequency counts. The
first thing you do is to create a data frame where the there is one line
for each body of text. In this case we have only one long string of text
this will be a one line data frame.

```{r}
library(tidytext)
library(tidyr)
text_df <- tibble(line = 1:length(lincoln_doc), text = lincoln_doc)

text_df
```

The next step is to breakup each of text lines (we have only 1) into
invdividual rows, each with it’s own line. We also want to count the
number of times that each word appears. This is known as **tokenizing**
the data frame.

```{r}
token_text <- text_df %>%
  unnest_tokens(word, text)

# Let's now count them

token_text %>% count(word,sort=TRUE)
```

But we need to get rid of the “stop words.” It’s a good thing that the
**tidytext** package has a way to filter out the common words that do
not significantly contribute to the meaning of the overall text. The
**stop_words** data frame is built into **tidytext**. Take a look to see
some of the words contained therein:

```{r}
data(stop_words)

# Sample 40 random stop words

stop_words %>% sample_n(40)
```

```{r}
# Now remove stop words from the document

tidy_text <- token_text %>%
  anti_join(stop_words)
```

```{r}
# This could also be done by the following. I point this out only because some people react
# negatively to "joins" although fully understanding what joins are can only help you since
# much of what the dplyr package does is based on SQL type joins. 

tidy_text <- token_text %>%
  filter(!word %in% stop_words$word)

tidy_text %>% count(word,sort=TRUE)
```

```{r}
tidy_text %>% count(word,sort=TRUE)
```

```{r}
tidy_text %>%
  count(word, sort = TRUE) %>%
  filter(n > 2) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```

## Back To PubMed Example

We have around 935 abstracts that we mess with based on our work using
the **easyPubMed** package

```{r}
# Create a data frame out of the cleaned up abstracts
library(tidytext)
library(dplyr)
text_df <- data_frame(line = 1:length(my_abstracts), text = my_abstracts)
token_text <- text_df %>%
  unnest_tokens(word, text)

# Many of these words aren't helpful 
token_text %>% count(total=word,sort=TRUE)
```

```{r}
# Now remove stop words
data(stop_words)
tidy_text <- token_text %>%
  anti_join(stop_words)

# This could also be done by the following. I point this out only because some people react
# negatively to "joins" although fully understanding what joins are can only help you since
# much of what the dplyr package does is based on SQL type joins. 

tidy_text <- token_text %>%
  filter(!word %in% stop_words$word)

# Arrange the text by descending word frequency 

tidy_text %>%
  count(word, sort = TRUE) 
```

Some of the most frequently occurring words are in fact “dialysis,”
“patients” so maybe we should consider them to be stop words also since
we already know quite well that the overall theme is, well, dialysis and
kidneys. There are also synonymns and abbreviations that are somewhat
redundant such as “pdd,”“pd,”“hhd” so let’s eliminate them also.

```{r}
tidy_text <- token_text %>%
   filter(!word %in% c(stop_words$word,"dialysis","patients","home","kidney",
                       "hemodialysis","haemodialysis","patient","hhd",
                       "pd","peritoneal","hd","renal","study","care",
                       "ci","chd","nhd","disease","treatment"))

tidy_text %>%
  count(word, sort = TRUE) 
```

```{r}
library(ggplot2)
tidy_text %>%
  count(word, sort = TRUE) %>%
  filter(n > 120) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```

Okay, it looks like there are numbers in there which might be useful. I
suspect that the “95” is probably associated with the idea of a
confidence interval. But there are other references to numbers.

```{r}
grep("^[0-9]{1,3}$",tidy_text$word,value=T)[1:20]
```

```{r}
tidy_text_nonum <- tidy_text[grep("^[0-9]{1,3}$",tidy_text$word,invert=TRUE),]
```

Okay well I think maybe we have some reasonable data to examine. As you
might have realized by now, manipulating data to get it “clean” can be
tedious and frustrating though it is an inevitable part of the process.

```{r}
tidy_text_nonum %>%
  count(word, sort = TRUE) %>%
  filter(n > 120) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```

## Sentiment Analysis

The next step is to explore what some of these words might mean. The
**tidytext** package has four dictionaries that help you figure out what
sentiment is being expressed by your data frame.

```{r}
# NRC Emotion Lexicon from Saif Mohammad and Peter Turney
get_sentiments("nrc") %>% sample_n(20)
```

```{r}
# the sentiment lexicon from Bing Liu and collaborators
get_sentiments("bing") %>% sample_n(20)
```

```{r}
# Tim Loughran and Bill McDonald
get_sentiments("loughran") %>% sample_n(20)
```

```{r}
# Pull out words that correspond to joy
nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

nrc_joy
```

So we will use the **nrc** sentiment dictionary to see the “sentiment”
expressed in our abstracts.

```{r}
bing_word_counts <- tidy_text_nonum %>% 
  inner_join(get_sentiments("nrc")) %>% 
  count(word,sentiment,sort=TRUE)
```

```{r}
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```

```{r}
library(wordcloud)
#

tidy_text_nonum %>%  
  count(word) %>%
  with(wordcloud(word,n,max.words=90,scale=c(4,.5),colors=brewer.pal(8,"Dark2")))
```

## BiGrams

Let’s look at bigrams. We need to go back to the cleaned abstracts and
pair words to get phrase that might be suggestive of some sentiment

```{r}
text_df <- data_frame(line = 1:length(my_abstracts), text = my_abstracts)
dialysis_bigrams <- text_df %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

dialysis_bigrams %>%
  count(bigram, sort = TRUE)
```

But we have to filter out stop words

```{r}
library(tidyr)
bigrams_sep <- dialysis_bigrams %>% 
  separate(bigram,c("word1","word2"),sep=" ")

stop_list <- c(stop_words$word,"dialysis","patients","home","kidney",
                       "hemodialysis","haemodialysis","treatment","patient","hhd",
                       "pd","peritoneal","hd","renal","study","care",
                       "ci","chd","nhd","esrd","lt","95","0.001")

bigrams_filtered <- bigrams_sep %>% 
  filter(!word1 %in% stop_list) %>%
  filter(!word2 %in% stop_list)

bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united %>%  count(bigram, sort = TRUE) %>% print(n=25)
```

```{r}
library(tidyquant)
bigram_counts %>%
  filter(n > 20) %>%
  ggplot(aes(x = reorder(word1, -n), y = reorder(word2, -n), fill = n)) +
    geom_tile(alpha = 0.8, color = "white") +
    scale_fill_gradientn(colours = c(palette_light()[[1]], palette_light()[[2]])) +
    coord_flip() +
    theme_tq() +
    theme(legend.position = "right") +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
    labs(x = "first word in pair",
         y = "second word in pair",
         title="BiGram Plot")
```

## Topic Modeling

In text mining, topic modeling serves as an unsupervised classification
method for such documents, akin to clustering in numeric data, where it
identifies natural groupings of items even when the specific categories
are not predefined.

One widely used approach for implementing a topic model is Latent
Dirichlet Allocation (LDA). LDA views each document as a blend of
various topics, and each topic as a blend of different words. This
methodology permits documents to share common themes, enabling them to
overlap in content rather than being distinctly separated into
predefined categories, resembling the fluidity of natural language
usage.

```{r}
library(topicmodels)

# Let's create a sparse matrix
sparse_mat <- tidy_text_nonum %>% cast_sparse(line,word)

# Let's perform LDA and select 3 topics
kd_lda <- LDA(sparse_mat,k=3,control=list(seed=123))

kd_lda
```

```{r}
kd_topics <- tidy(kd_lda, matrix="beta")
head(kd_topics,10)
```

```{r}
kd_top_terms <- kd_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 12) %>% 
  ungroup() %>%
  arrange(topic, -beta)

kd_top_terms
```

```{r}
kd_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```

```         
```
